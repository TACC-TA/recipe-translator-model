{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import FastText,fasttext\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import math\n",
    "import re\n",
    "from string import digits\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_embeding_path = './embeddings/lematized_embedding_v1.model'\n",
    "qc_embeddings = FastText.load(qc_embeding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_embeding_path = './embeddings/cc.es.300.bin'\n",
    "es_embeddings = fasttext.load_facebook_vectors(es_embeding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quechua</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ch’arwi</td>\n",
       "      <td>Desorden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Runa</td>\n",
       "      <td>Persona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chanin</td>\n",
       "      <td>Precio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puka</td>\n",
       "      <td>Rojo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hallp’a</td>\n",
       "      <td>Tierra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>Runakuna manan mallkikunata tarpunkuchu chaymi...</td>\n",
       "      <td>La personas no plantan árboles es uno de los f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>Erqe mamanpa wañusqanmanta waqan.</td>\n",
       "      <td>El niño llora porque su madre murió.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>Huk wayna chakamanta wukchuyukusqa hinaspa wañ...</td>\n",
       "      <td>Un joven se lanzó del puente y murió.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>Mama churin mana wasinman kutimusqanmanta waqan.</td>\n",
       "      <td>La madre llora porque su hijo no vuelve a la c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>Erqe anchata karru alqonta saruruqtin waqasqa.</td>\n",
       "      <td>El niño lloró mucho porque el carro lo atropel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2071 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                quechua  \\\n",
       "0                                               Ch’arwi   \n",
       "1                                                  Runa   \n",
       "2                                                Chanin   \n",
       "3                                                  Puka   \n",
       "4                                               Hallp’a   \n",
       "...                                                 ...   \n",
       "2066  Runakuna manan mallkikunata tarpunkuchu chaymi...   \n",
       "2067                  Erqe mamanpa wañusqanmanta waqan.   \n",
       "2068  Huk wayna chakamanta wukchuyukusqa hinaspa wañ...   \n",
       "2069   Mama churin mana wasinman kutimusqanmanta waqan.   \n",
       "2070     Erqe anchata karru alqonta saruruqtin waqasqa.   \n",
       "\n",
       "                                                spanish  \n",
       "0                                              Desorden  \n",
       "1                                               Persona  \n",
       "2                                                Precio  \n",
       "3                                                  Rojo  \n",
       "4                                                Tierra  \n",
       "...                                                 ...  \n",
       "2066  La personas no plantan árboles es uno de los f...  \n",
       "2067               El niño llora porque su madre murió.  \n",
       "2068              Un joven se lanzó del puente y murió.  \n",
       "2069  La madre llora porque su hijo no vuelve a la c...  \n",
       "2070  El niño lloró mucho porque el carro lo atropel...  \n",
       "\n",
       "[2071 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns=['quechua', 'spanish'])\n",
    "dataset['quechua'] = pd.read_csv('./train_data/QuechuaCollaoCorpus.csv')['Quechua']\n",
    "dataset['spanish'] = pd.read_csv('./train_data/QuechuaCollaoCorpus.csv')['Traducción']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna(inplace=True)\n",
    "dataset.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    num_digits = str.maketrans('', '', digits)\n",
    "\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(\" +\", \" \", sentence)\n",
    "    sentence = re.sub(\" . \", \"\", sentence)\n",
    "    sentence = sentence.translate(num_digits)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)\n",
    "    sentence = sentence.rstrip().strip()\n",
    "    \n",
    "    # Remove the final dot if it exists\n",
    "    if sentence.endswith('.'):\n",
    "        sentence = sentence[:-1].strip()\n",
    "    sentence = sentence.replace('’',\"'\")\n",
    "    sentence = 'start_ ' + sentence + ' _end'  # importante\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def clean_df(df):\n",
    "    df['quechua'] = df['quechua'].apply(lambda x: preprocess_sentence(x))\n",
    "    df['spanish'] = df['spanish'].apply(lambda x: preprocess_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quechua</th>\n",
       "      <th>spanish</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start_ ch'arwi _end</td>\n",
       "      <td>start_ desorden _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>start_ runa _end</td>\n",
       "      <td>start_ persona _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start_ chanin _end</td>\n",
       "      <td>start_ precio _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>start_ puka _end</td>\n",
       "      <td>start_ rojo _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start_ hallp'a _end</td>\n",
       "      <td>start_ tierra _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2064</th>\n",
       "      <td>start_ runakuna manan mallkikunata tarpunkuchu...</td>\n",
       "      <td>start_ la personas no plantan árboles es uno d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2065</th>\n",
       "      <td>start_ erqe mamanpa wañusqanmanta waqan _end</td>\n",
       "      <td>start_ el niño llora porque su madre murió _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>start_ huk wayna chakamanta wukchuyukusqa hina...</td>\n",
       "      <td>start_ un joven se lanzó del puentemurió _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>start_ mama churin mana wasinman kutimusqanman...</td>\n",
       "      <td>start_ la madre llora porque su hijo no vuelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2068</th>\n",
       "      <td>start_ erqe anchata karru alqonta saruruqtin w...</td>\n",
       "      <td>start_ el niño lloró mucho porque el carro lo ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2069 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                quechua  \\\n",
       "0                                   start_ ch'arwi _end   \n",
       "1                                      start_ runa _end   \n",
       "2                                    start_ chanin _end   \n",
       "3                                      start_ puka _end   \n",
       "4                                   start_ hallp'a _end   \n",
       "...                                                 ...   \n",
       "2064  start_ runakuna manan mallkikunata tarpunkuchu...   \n",
       "2065       start_ erqe mamanpa wañusqanmanta waqan _end   \n",
       "2066  start_ huk wayna chakamanta wukchuyukusqa hina...   \n",
       "2067  start_ mama churin mana wasinman kutimusqanman...   \n",
       "2068  start_ erqe anchata karru alqonta saruruqtin w...   \n",
       "\n",
       "                                                spanish  \n",
       "0                                  start_ desorden _end  \n",
       "1                                   start_ persona _end  \n",
       "2                                    start_ precio _end  \n",
       "3                                      start_ rojo _end  \n",
       "4                                    start_ tierra _end  \n",
       "...                                                 ...  \n",
       "2064  start_ la personas no plantan árboles es uno d...  \n",
       "2065    start_ el niño llora porque su madre murió _end  \n",
       "2066      start_ un joven se lanzó del puentemurió _end  \n",
       "2067  start_ la madre llora porque su hijo no vuelve...  \n",
       "2068  start_ el niño lloró mucho porque el carro lo ...  \n",
       "\n",
       "[2069 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1006, 969980)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc_embeddings.build_vocab(dataset['quechua'].apply(lambda x: x.split()).tolist(), update=True)\n",
    "qc_embeddings.train(dataset['quechua'], total_examples=qc_embeddings.corpus_count, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_word_dict = {\n",
    "    '_pad_': np.zeros(50, dtype=np.float32),\n",
    "}\n",
    "for words in dataset['quechua']:\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        if not word in qc_word_dict:\n",
    "            qc_word_dict[word] = qc_embeddings.wv[word]\n",
    "\n",
    "sp_word_dict = {\n",
    "    '_pad_': np.zeros(300, dtype=np.float32),\n",
    "}\n",
    "\n",
    "for words in dataset['spanish']:\n",
    "    words = words.split()\n",
    "    for word in words:\n",
    "        if not word in sp_word_dict:\n",
    "            sp_word_dict[word] = es_embeddings[word]\n",
    "\n",
    "qc_word_index = {word: i for i, word in enumerate(qc_word_dict.keys())}\n",
    "sp_word_index = {word: i for i, word in enumerate(sp_word_dict.keys())}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc_max_len = dataset['quechua'].apply(len).max()\n",
    "sp_max_len = dataset['spanish'].apply(len).max()\n",
    "sp_max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TranslationTransformerModel(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, \n",
    "                 dim_feedforward, max_seq_length, dropout, qc_word_dict, sp_word_dict):\n",
    "        super(TranslationTransformerModel, self).__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "                                    nn.Embedding.from_pretrained(torch.tensor(list(sp_word_dict.values()))),#300 dim\n",
    "                                    nn.Linear(300, d_model))#50 dim\n",
    "        self.decoder = nn.Embedding.from_pretrained(torch.tensor(list(qc_word_dict.values()))) #50 dim\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout, max_seq_length)\n",
    "        \n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, len(qc_word_dict))\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "            src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.decoder(tgt) * math.sqrt(self.d_model)\n",
    "    \n",
    "        src = self.pos_encoder(src)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.generate_square_subsequent_mask(tgt.size(1))\n",
    "        \n",
    "        if src_mask is None:\n",
    "            src_mask = torch.zeros(src.size(1), src.size(1),device=device).type(torch.bool) \n",
    "        \n",
    "        \n",
    "        output = self.transformer(src, tgt, src_mask, tgt_mask, memory_mask,\n",
    "                              src_key_padding_mask, tgt_key_padding_mask, memory_key_padding_mask)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, sp_max_len, qc_max_len):\n",
    "        self.dataset = dataset\n",
    "        self.sp_max_len = sp_max_len\n",
    "        self.qc_max_len = qc_max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sp = [sp_word_index[word] for word in self.dataset['spanish'][idx].split()]\n",
    "        qc = [qc_word_index[word] for word in self.dataset['quechua'][idx].split()]\n",
    "        \n",
    "        sp = sp + [0] * (self.sp_max_len - len(sp))\n",
    "        qc = qc + [0] * (self.qc_max_len - len(qc))\n",
    "        \n",
    "        return torch.tensor(sp, dtype=torch.long), torch.tensor(qc, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TranslationDataset(dataset, sp_max_len, qc_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabzio/.conda/envs/tacc/lib/python3.12/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.num_heads is odd\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = TranslationTransformerModel(d_model=50, nhead=5, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, max_seq_length=sp_max_len, dropout=0.1, qc_word_dict=qc_word_dict, sp_word_dict=sp_word_dict).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 141.60622559653387\n",
      "Epoch 1: 139.32030396991306\n",
      "Epoch 2: 133.27734602822198\n",
      "Epoch 3: 131.90463903215198\n",
      "Epoch 4: 131.18657933341132\n",
      "Epoch 5: 130.29598167207507\n",
      "Epoch 6: 129.57277022467719\n",
      "Epoch 7: 129.58455827501086\n",
      "Epoch 8: 129.3541211022271\n",
      "Epoch 9: 129.09915187623767\n"
     ]
    }
   ],
   "source": [
    "def create_padding_mask(seq, pad_idx=0):\n",
    "    return (seq == pad_idx)\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for i, (src, tgt) in enumerate(train_dl):\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        src_padding_mask = create_padding_mask(src)\n",
    "        tgt_padding_mask = create_padding_mask(tgt)\n",
    "        memory_padding_mask = src_padding_mask.clone()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src,tgt,src_key_padding_mask=src_padding_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
    "        logits = output.view(-1, len(qc_word_index))\n",
    "        loss = criterion(logits, tgt.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f'Epoch {epoch}: {total_loss / len(src)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing\n",
    "\n",
    "def translate_sentence(sentence, model, qc_word_dict, sp_word_dict, qc_word_index, sp_word_index, qc_max_len, sp_max_len):\n",
    "    model.eval()\n",
    "    words = sentence.split()\n",
    "    sp = [sp_word_index[word] for word in words]\n",
    "    sp = sp + [0] * (sp_max_len - len(sp))\n",
    "    sp = torch.tensor(sp, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    qc = torch.tensor([1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    qc_padded = torch.tensor([0] * qc_max_len, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    for i in range(qc_max_len):\n",
    "        output = model(sp, qc_padded)\n",
    "        output = output[-1,:].unsqueeze(0)\n",
    "        predicted_index = output.argmax(2)\n",
    "        qc = torch.cat((qc, predicted_index), dim=1)\n",
    "        if output[0, -1] == 2:\n",
    "            break\n",
    "    \n",
    "    qc = qc.squeeze().detach().cpu().numpy()\n",
    "    qc = [list(qc_word_index.keys())[list(qc_word_index.values()).index(i)] for i in qc]\n",
    "    qc = ' '.join(qc)\n",
    "    return qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_ estaba en mi casa con mi familia _end'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['spanish'][1253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_ wasiypin aylluywan kushka karani _end'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['quechua'][1253]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m translate_sentence(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspanish\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1253\u001b[39m],model,qc_word_dict,sp_word_dict,qc_word_index,sp_word_index,qc_max_len,sp_max_len)\n",
      "Cell \u001b[0;32mIn[106], line 17\u001b[0m, in \u001b[0;36mtranslate_sentence\u001b[0;34m(sentence, model, qc_word_dict, sp_word_dict, qc_word_index, sp_word_index, qc_max_len, sp_max_len)\u001b[0m\n\u001b[1;32m     15\u001b[0m     predicted_index \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     16\u001b[0m     qc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((qc, predicted_index), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     20\u001b[0m qc \u001b[38;5;241m=\u001b[39m qc\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "translate_sentence(dataset['spanish'][1253],model,qc_word_dict,sp_word_dict,qc_word_index,sp_word_index,qc_max_len,sp_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bleu score\n",
    "\n",
    "def calculate_bleu_score(model, dataset, qc_word_dict, sp_word_dict, qc_word_index, sp_word_index, qc_max_len, sp_max_len):\n",
    "    bleu_score = 0\n",
    "    for i in range(len(dataset)):\n",
    "        sp = dataset['spanish'][i]\n",
    "        qc = dataset['quechua'][i]\n",
    "        translated_qc = translate_sentence(sp, model, qc_word_dict, sp_word_dict, qc_word_index, sp_word_index, qc_max_len, sp_max_len)\n",
    "        bleu_score += sentence_bleu([qc.split()], translated_qc.split())\n",
    "    return bleu_score / len(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "score = calculate_bleu_score(model, dataset, qc_word_dict, sp_word_dict, qc_word_index, sp_word_index, qc_max_len, sp_max_len)\n",
    "score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tacc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
